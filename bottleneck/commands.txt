# compression for maxout and bottleneck commands
MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 fairseq-train --task language_modeling --share-decoder-input-output-embed --sample-break-mode none --ddp-backend=no_c10d --log-format simple --log-interval 50 --fp16 --keep-best-checkpoints 1 --no-epoch-checkpoints --keep-interval-updates 1 --distributed-port 12598 --distributed-world-size 24 --valid-subset valid --arch transformer_lm --weight-decay 0.0 --validate-interval-updates 1000 --save-interval-updates 1000 --lr-scheduler cosine --fp16-no-flatten-grads  --min-loss-scale 1e-10 --fp16-scale-window 250 --valid-subset valid_wiki,valid,valid_1b,valid_lambada,valid_wiki2,valid_ptb --optimizer adam --decoder-embed-dim 1024 --decoder-ffn-embed-dim 4096 --decoder-attention-heads 16 --decoder-input-dim 1024 --decoder-output-dim 1024  --ff-block bottleneck --maxout 1 --scale-factor 2   --num-stages 2  --clip-norm 0.1  --adam-betas '(0.9, 0.995)'  --adam-eps 1e-07  --decoder-layers 16 --dropout 0.0 --attention-dropout 0.0 --relu-dropout 0.0  --max-tokens 2048 --update-freq 5 --tokens-per-sample 512 --max-update 36144  --warmup-updates 5000  /mmfs1/home/anonymous/data/openwebtext   --weight-decay 0.0 --lr 0.0019541575181093226 --warmup-init-lr 0.0  --seed 4 --save-dir /gscratch/scrubbed/anonymous/compression/gpt2_8bit/faf8fc90ea0af5a4ce13808aa9d9b455 
MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 fairseq-train --task language_modeling --share-decoder-input-output-embed --sample-break-mode none --ddp-backend=no_c10d --log-format simple --log-interval 50 --fp16 --keep-best-checkpoints 1 --no-epoch-checkpoints --keep-interval-updates 1 --distributed-port 12599 --distributed-world-size 24 --valid-subset valid --arch transformer_lm --weight-decay 0.0 --validate-interval-updates 1000 --save-interval-updates 1000 --lr-scheduler cosine --fp16-no-flatten-grads  --min-loss-scale 1e-10 --fp16-scale-window 250 --valid-subset valid_wiki,valid,valid_1b,valid_lambada,valid_wiki2,valid_ptb --optimizer adam --decoder-embed-dim 1024 --decoder-ffn-embed-dim 4096 --decoder-attention-heads 16 --decoder-input-dim 1024 --decoder-output-dim 1024  --ff-block bottleneck --maxout 1 --scale-factor 2   --num-stages 4  --clip-norm 0.1  --adam-betas '(0.9, 0.995)'  --adam-eps 1e-07  --decoder-layers 16 --dropout 0.0 --attention-dropout 0.0 --relu-dropout 0.0  --max-tokens 2048 --update-freq 5 --tokens-per-sample 512 --max-update 36144  --warmup-updates 5000  /mmfs1/home/anonymous/data/openwebtext   --weight-decay 0.0 --lr 0.0019541575181093226 --warmup-init-lr 0.0  --seed 4 --save-dir /gscratch/scrubbed/anonymous/compression/gpt2_8bit/a2c89f5778531a365d59dbd3651f67ae 
MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 fairseq-train --task language_modeling --share-decoder-input-output-embed --sample-break-mode none --ddp-backend=no_c10d --log-format simple --log-interval 50 --fp16 --keep-best-checkpoints 1 --no-epoch-checkpoints --keep-interval-updates 1 --distributed-port 12600 --distributed-world-size 24 --valid-subset valid --arch transformer_lm --weight-decay 0.0 --validate-interval-updates 1000 --save-interval-updates 1000 --lr-scheduler cosine --fp16-no-flatten-grads  --min-loss-scale 1e-10 --fp16-scale-window 250 --valid-subset valid_wiki,valid,valid_1b,valid_lambada,valid_wiki2,valid_ptb --optimizer adam --decoder-embed-dim 1024 --decoder-ffn-embed-dim 4096 --decoder-attention-heads 16 --decoder-input-dim 1024 --decoder-output-dim 1024  --ff-block bottleneck --maxout 1 --scale-factor 4   --num-stages 2  --clip-norm 0.1  --adam-betas '(0.9, 0.995)'  --adam-eps 1e-07  --decoder-layers 16 --dropout 0.0 --attention-dropout 0.0 --relu-dropout 0.0  --max-tokens 2048 --update-freq 5 --tokens-per-sample 512 --max-update 36144  --warmup-updates 5000  /mmfs1/home/anonymous/data/openwebtext   --weight-decay 0.0 --lr 0.0019541575181093226 --warmup-init-lr 0.0  --seed 4 --save-dir /gscratch/scrubbed/anonymous/compression/gpt2_8bit/94d93044553b369a41b98906adc90115 
MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 fairseq-train --task language_modeling --share-decoder-input-output-embed --sample-break-mode none --ddp-backend=no_c10d --log-format simple --log-interval 50 --fp16 --keep-best-checkpoints 1 --no-epoch-checkpoints --keep-interval-updates 1 --distributed-port 12601 --distributed-world-size 24 --valid-subset valid --arch transformer_lm --weight-decay 0.0 --validate-interval-updates 1000 --save-interval-updates 1000 --lr-scheduler cosine --fp16-no-flatten-grads  --min-loss-scale 1e-10 --fp16-scale-window 250 --valid-subset valid_wiki,valid,valid_1b,valid_lambada,valid_wiki2,valid_ptb --optimizer adam --decoder-embed-dim 1024 --decoder-ffn-embed-dim 4096 --decoder-attention-heads 16 --decoder-input-dim 1024 --decoder-output-dim 1024  --ff-block bottleneck --maxout 1 --scale-factor 4   --num-stages 4  --clip-norm 0.1  --adam-betas '(0.9, 0.995)'  --adam-eps 1e-07  --decoder-layers 16 --dropout 0.0 --attention-dropout 0.0 --relu-dropout 0.0  --max-tokens 2048 --update-freq 5 --tokens-per-sample 512 --max-update 36144  --warmup-updates 5000  /mmfs1/home/anonymous/data/openwebtext   --weight-decay 0.0 --lr 0.0019541575181093226 --warmup-init-lr 0.0  --seed 4 --save-dir /gscratch/scrubbed/anonymous/compression/gpt2_8bit/1c864d32ad2ff3b3861d7dcca85ecff9 
MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 fairseq-train --task language_modeling --share-decoder-input-output-embed --sample-break-mode none --ddp-backend=no_c10d --log-format simple --log-interval 50 --fp16 --keep-best-checkpoints 1 --no-epoch-checkpoints --keep-interval-updates 1 --distributed-port 12602 --distributed-world-size 24 --valid-subset valid --arch transformer_lm --weight-decay 0.0 --validate-interval-updates 1000 --save-interval-updates 1000 --lr-scheduler cosine --fp16-no-flatten-grads  --min-loss-scale 1e-10 --fp16-scale-window 250 --valid-subset valid_wiki,valid,valid_1b,valid_lambada,valid_wiki2,valid_ptb --optimizer adam --decoder-embed-dim 1024 --decoder-ffn-embed-dim 4096 --decoder-attention-heads 16 --decoder-input-dim 1024 --decoder-output-dim 1024  --ff-block bottleneck --maxout 2 --scale-factor 1   --num-stages 2  --clip-norm 0.1  --adam-betas '(0.9, 0.995)'  --adam-eps 1e-07  --decoder-layers 16 --dropout 0.0 --attention-dropout 0.0 --relu-dropout 0.0  --max-tokens 2048 --update-freq 5 --tokens-per-sample 512 --max-update 36144  --warmup-updates 5000  /mmfs1/home/anonymous/data/openwebtext   --weight-decay 0.0 --lr 0.0019541575181093226 --warmup-init-lr 0.0  --seed 4 --save-dir /gscratch/scrubbed/anonymous/compression/gpt2_8bit/76fec005abb2ffe809ea109ae232f532 
MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 fairseq-train --task language_modeling --share-decoder-input-output-embed --sample-break-mode none --ddp-backend=no_c10d --log-format simple --log-interval 50 --fp16 --keep-best-checkpoints 1 --no-epoch-checkpoints --keep-interval-updates 1 --distributed-port 12603 --distributed-world-size 24 --valid-subset valid --arch transformer_lm --weight-decay 0.0 --validate-interval-updates 1000 --save-interval-updates 1000 --lr-scheduler cosine --fp16-no-flatten-grads  --min-loss-scale 1e-10 --fp16-scale-window 250 --valid-subset valid_wiki,valid,valid_1b,valid_lambada,valid_wiki2,valid_ptb --optimizer adam --decoder-embed-dim 1024 --decoder-ffn-embed-dim 4096 --decoder-attention-heads 16 --decoder-input-dim 1024 --decoder-output-dim 1024  --ff-block bottleneck --maxout 2 --scale-factor 1   --num-stages 4  --clip-norm 0.1  --adam-betas '(0.9, 0.995)'  --adam-eps 1e-07  --decoder-layers 16 --dropout 0.0 --attention-dropout 0.0 --relu-dropout 0.0  --max-tokens 2048 --update-freq 5 --tokens-per-sample 512 --max-update 36144  --warmup-updates 5000  /mmfs1/home/anonymous/data/openwebtext   --weight-decay 0.0 --lr 0.0019541575181093226 --warmup-init-lr 0.0  --seed 4 --save-dir /gscratch/scrubbed/anonymous/compression/gpt2_8bit/36b133c536851770b7668079fecb9419 
MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 fairseq-train --task language_modeling --share-decoder-input-output-embed --sample-break-mode none --ddp-backend=no_c10d --log-format simple --log-interval 50 --fp16 --keep-best-checkpoints 1 --no-epoch-checkpoints --keep-interval-updates 1 --distributed-port 12604 --distributed-world-size 24 --valid-subset valid --arch transformer_lm --weight-decay 0.0 --validate-interval-updates 1000 --save-interval-updates 1000 --lr-scheduler cosine --fp16-no-flatten-grads  --min-loss-scale 1e-10 --fp16-scale-window 250 --valid-subset valid_wiki,valid,valid_1b,valid_lambada,valid_wiki2,valid_ptb --optimizer adam --decoder-embed-dim 1024 --decoder-ffn-embed-dim 4096 --decoder-attention-heads 16 --decoder-input-dim 1024 --decoder-output-dim 1024  --ff-block bottleneck --maxout 4 --scale-factor 1   --num-stages 2  --clip-norm 0.1  --adam-betas '(0.9, 0.995)'  --adam-eps 1e-07  --decoder-layers 16 --dropout 0.0 --attention-dropout 0.0 --relu-dropout 0.0  --max-tokens 2048 --update-freq 5 --tokens-per-sample 512 --max-update 36144  --warmup-updates 5000  /mmfs1/home/anonymous/data/openwebtext   --weight-decay 0.0 --lr 0.0019541575181093226 --warmup-init-lr 0.0  --seed 4 --save-dir /gscratch/scrubbed/anonymous/compression/gpt2_8bit/c799a795a59d873b7df8d895bf94be46 
MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 fairseq-train --task language_modeling --share-decoder-input-output-embed --sample-break-mode none --ddp-backend=no_c10d --log-format simple --log-interval 50 --fp16 --keep-best-checkpoints 1 --no-epoch-checkpoints --keep-interval-updates 1 --distributed-port 12605 --distributed-world-size 24 --valid-subset valid --arch transformer_lm --weight-decay 0.0 --validate-interval-updates 1000 --save-interval-updates 1000 --lr-scheduler cosine --fp16-no-flatten-grads  --min-loss-scale 1e-10 --fp16-scale-window 250 --valid-subset valid_wiki,valid,valid_1b,valid_lambada,valid_wiki2,valid_ptb --optimizer adam --decoder-embed-dim 1024 --decoder-ffn-embed-dim 4096 --decoder-attention-heads 16 --decoder-input-dim 1024 --decoder-output-dim 1024  --ff-block bottleneck --maxout 4 --scale-factor 1   --num-stages 4  --clip-norm 0.1  --adam-betas '(0.9, 0.995)'  --adam-eps 1e-07  --decoder-layers 16 --dropout 0.0 --attention-dropout 0.0 --relu-dropout 0.0  --max-tokens 2048 --update-freq 5 --tokens-per-sample 512 --max-update 36144  --warmup-updates 5000  /mmfs1/home/anonymous/data/openwebtext   --weight-decay 0.0 --lr 0.0019541575181093226 --warmup-init-lr 0.0  --seed 4 --save-dir /gscratch/scrubbed/anonymous/compression/gpt2_8bit/83555e42e9d0b3df805c187e6b20a08b 

# baseline model 
MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 fairseq-train --task language_modeling --share-decoder-input-output-embed --sample-break-mode none --ddp-backend=no_c10d --log-format simple --log-interval 50 --fp16 --keep-best-checkpoints 1 --no-epoch-checkpoints --keep-interval-updates 1 --distributed-port 12598 --distributed-world-size 24 --valid-subset valid --arch transformer_lm --weight-decay 0.0 --validate-interval-updates 1000 --save-interval-updates 1000 --lr-scheduler cosine --fp16-no-flatten-grads  --min-loss-scale 1e-10 --fp16-scale-window 250 --valid-subset valid_wiki,valid,valid_1b,valid_lambada,valid_wiki2,valid_ptb --optimizer adam --decoder-embed-dim 1024 --decoder-ffn-embed-dim 4096 --decoder-attention-heads 16 --decoder-input-dim 1024 --decoder-output-dim 1024   --clip-norm 0.1  --adam-betas '(0.9, 0.995)'  --adam-eps 1e-07  --decoder-layers 16 --dropout 0.0 --attention-dropout 0.0 --relu-dropout 0.0  --max-tokens 2048 --update-freq 5 --tokens-per-sample 512 --max-update 36144  --warmup-updates 5000  /mmfs1/home/anonymous/data/openwebtext   --weight-decay 0.0 --lr 0.0019541575181093226 --warmup-init-lr 0.0  --seed 4 --save-dir /gscratch/scrubbed/anonymous/compression/gpt2_8bit/703375aa8459043d79abedbe4e0d6987
